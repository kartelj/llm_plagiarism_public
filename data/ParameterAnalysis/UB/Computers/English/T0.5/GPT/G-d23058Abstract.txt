Emotional speech, a phenomenon seldom addressed in current speaker recognition research, presents a significant challenge due to the variance in a speaker's emotional state during system training and usage. Our study analyzed various speaker modeling and classification algorithms. We evaluated Gaussian mixture models, a fundamental technique in speaker recognition, and i-vectors, a cutting-edge technique for commercial use. Our findings indicated that system robustness in recognizing speakers from both neutral and emotionally charged speech can be improved by varying the emotional content of speech and the number of sentences used for speaker model training. We also explored variations in model configuration by creating three models for each speaker, each trained with appropriate emotional speech grouped by emotion valence. The final aspect of our research involved varying the speaker model structure by determining the number of components in the Gaussian mixture based on subtractive clustering. The number of components was automatically derived from the training utterances for each speaker using subtractive clustering. We induced theoretical dependencies of subtractive clustering parameters and the number of feature vectors in the training sample, feature vector dimensionality, and their distribution. Our results suggest that a speaker can be effectively modeled with fewer components in the Gaussian mixture, paving the way for further research into the application of subtractive clustering for speaker model training.