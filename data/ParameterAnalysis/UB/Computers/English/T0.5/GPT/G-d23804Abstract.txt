The successful incorporation of emotional intelligence into advanced artificial intelligence systems hinges on the dependable identification of emotional states. The paralinguistic aspects of speech are notably crucial in conveying information about the speaker's emotional state. This paper conducts a comparative study of the commonly used speech signal features and classification methods for the automatic recognition of speaker's emotional states. Following this, we explore ways to enhance the performance of systems for automatic speech emotion recognition. We improved discrete hidden Markov models using the QQ plot to determine codevectors for vector quantization, and also considered additional model enhancements. We investigated how to more accurately represent the speech signal, extending the analysis to a wide range of features from various groups. The creation of large feature sets necessitates dimensionality reduction, where we analyzed a method based on the Fibonacci sequence, in addition to established methods. We also discuss how to merge the benefits of different approaches into a single system for automatic speech emotion recognition, proposing a parallel multiclassifier structure with a combinatorial rule that uses information about classifiers' characteristics in addition to individual ensemble classifiers' classification results. Lastly, we propose the automatic creation of a classifier ensemble of any size using dimensionality reduction based on the Fibonacci sequence.