The successful assimilation of emotional intelligence into advanced artificial intelligence systems is contingent upon the dependable identification of emotional states. In this context, the paralinguistic constituents of speech emerge as a crucial source of emotional data. This study performs a comparative analysis of commonly used speech signal features and classification techniques used in automatic speaker emotional state recognition. Opportunities to enhance system performances for automatic speech emotion recognition are explored. The QQ plot was utilized to improve discrete hidden Markov models by determining codevectors for vector quantization with additional model enhancements discussed. The potential for a more accurate representation of the speech signal is examined, extending the analysis to include numerous features across various groups. The generation of large feature sets necessitates dimensionality reduction, where a method revolving around the Fibonacci number sequence was studied. Finally, we delve into merging the benefits of various methods into one system for automatic speech emotion detection proposing a parallel multiclassifier system incorporating a combinatorial rule. It utilizes not only individual ensemble classifier results but also data on classifier characteristics. The proposition also includes the automation of arbitrary ensemble classifier creation using dimensionality reduction dependent on the Fibonacci number sequence.