Statistical methodologies utilized for natural language processing generally need a significant volume of labeled data, and often additional language tools, thereby making them less suitable for resource-restricted settings. This dissertation introduces a strategy for devising statistical models in the semantic processing of languages with limited resources. For such languages, it's not only the existing language resources that are restricted, but the ability to create new datasets and dedicated tools and algorithms is also scant. The suggested strategy is specifically designed for short forms of text, considering their widespread use in digital communication and the complex nature of their semantic processing. This approach comprises all aspects of generating statistical models, from text content collection, to data annotation, and the development, training, and analysis of machine learning models. Its application is extensively exemplified through two semantic tasks â€“ sentiment analysis and semantic textual similarity. The Serbian language is used as a case study of languages with limited resources, though this strategy can extend to other such languages. Beyond presenting a generalized strategy, this dissertation offers the creation of a novel, adaptable short-text sentiment annotation system, a new cost-benefit metric for annotation, and several fresh models for semantic textual similarity. It also reveals the first Serbian annotated datasets of short texts available to the public for sentiment analysis and semantic textual similarity, along with the creation and scrutiny of a variety of models on these tasks. It wraps up with a maiden comparative analysis of a range of morphological normalization tools on Serbian short texts.