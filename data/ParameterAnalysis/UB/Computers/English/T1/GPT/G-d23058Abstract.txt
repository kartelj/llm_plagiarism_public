Modern speaker recognition research doesn't often consider the impact of emotional speech. This is a significant challenge for speaker recognition systems, due to shifts in a speaker's emotional state during system training and usage. Our research evaluated various speaker modeling and classification algorithms, including Gaussian mixture models and i-vectors. Our findings indicated that by adjusting the emotional content of speech used in training, as well as the number of sentences, system robustness is improved. We also examined the efficacy of creating three separate models for each speaker that were all trained on specific emotional speech. Lastly, our research involved making modifications to speaker model structure by automatically determining the number of components in a Gaussian mixture based on subtractive clustering. We discovered that theoretically, the parameters of subtractive clustering, number of vectors and their distribution, can effectively be modeled with a reduced number of components in the Gaussian mixture. This realization paves the way for future research on the use of subtractive clustering in speaker model training.