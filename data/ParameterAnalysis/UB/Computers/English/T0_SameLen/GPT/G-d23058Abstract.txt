Emotional speech phenomena are seldom incorporated into contemporary speaker recognition research. The primary obstacle for a speaker recognition system is the disparity in a speaker's emotional state during system training and usage. We analyzed various speaker modeling and classification algorithms in our study. We evaluated Gaussian mixture models, a basic technique in speaker recognition, and i-vectors, a cutting-edge technique for commercial applications, in our experiments. It was demonstrated that varying the emotional content of speech and the number of sentences used for speaker model training enhances system robustness in recognizing speakers from both neutral and emotionally affected speech. Additionally, we explored model configuration variation by creating three models for each speaker. Each model was trained with suitable emotional speech categorized by emotion valence. The final part of the research focused on varying speaker model structure by determining the number of components in the Gaussian mixture based on subtractive clustering. The number of components was automatically determined from the training utterances for each speaker using subtractive clustering. Theoretical dependencies of subtractive clustering parameters and the number of feature vectors in the training sample, feature vector dimensionality, and their distribution were established. The findings indicated that a speaker can be successfully modeled with fewer components in the Gaussian mixture, paving the way for further research into the application of subtractive clustering for speaker model training.