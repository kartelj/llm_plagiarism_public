The successful incorporation of emotional intelligence into advanced artificial intelligence systems hinges on the dependable identification of emotional states, with the paralinguistic elements of speech emerging as a particularly important source of information about the speaker's emotional state. This paper conducts a comparative study of the speech signal characteristics and classification techniques most commonly employed for the automatic recognition of speakers' emotional states, followed by a discussion on potential enhancements to the performance of systems for automatic speech emotion recognition. Discrete hidden Markov models were enhanced using the QQ plot to identify the codevectors for vector quantization, and further model enhancements were also explored. The potential for a more accurate representation of the speech signal was investigated, with the analysis encompassing a wide range of features from various groups. The creation of large feature sets necessitates dimensionality reduction, where an alternative method based on the Fibonacci number sequence was examined, along with established methods. Lastly, the potential for merging the benefits of different strategies into a unified system for automatic speech emotion recognition is discussed, proposing a parallel multiclassifier structure with a combinatorial rule that utilizes information about classifiers' characteristics in addition to the classification results of individual ensemble classifiers. A suggestion is also provided for the automatic creation of a classifier ensemble of any size using dimensionality reduction based on the Fibonacci number sequence.