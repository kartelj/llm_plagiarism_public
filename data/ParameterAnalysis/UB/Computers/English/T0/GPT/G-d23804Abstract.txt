The successful incorporation of emotional intelligence into advanced artificial intelligence systems hinges on the accurate identification of emotional states. This is particularly true when it comes to the paralinguistic elements of speech, which provide crucial information about the speaker's emotional state. This paper conducts a comparative study of the speech signal features and classification methods most commonly used for the automatic recognition of speakers' emotional states. It then explores ways to enhance the performance of these automatic speech emotion recognition systems. Discrete hidden Markov models were enhanced using the QQ plot to identify the codevectors for vector quantization, and other potential model improvements were also discussed. The paper also investigates how to more accurately represent the speech signal, extending the analysis to a wide range of features from various groups. The creation of large feature sets necessitates dimensionality reduction, and an alternative method based on the Fibonacci number sequence was examined, along with established methods. The paper concludes by considering how to combine the strengths of different approaches into a single system for automatic speech emotion recognition. A parallel multiclassifier structure is proposed, which uses a combinatorial rule that takes into account not only the classification results of individual ensemble classifiers, but also information about the classifiers' characteristics. The paper also proposes an automatic method for creating an ensemble of classifiers of any size, using dimensionality reduction based on the Fibonacci number sequence.