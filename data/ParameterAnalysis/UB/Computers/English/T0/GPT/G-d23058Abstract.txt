The study of emotional speech is not often incorporated into current speaker recognition research. The primary difficulty for a speaker recognition system lies in the variance of a speaker's emotional state during system training and usage. Our research analyzed various speaker modeling and classification algorithms. We evaluated Gaussian mixture models, a basic technique in speaker recognition, and i-vectors, a cutting-edge technique for commercial use. Our findings indicated that varying the emotional content of speech and the number of sentences used for speaker model training enhances system robustness in recognizing speakers from both neutral and emotionally charged speech. We also explored model configuration variation by creating three models for each speaker, each trained with emotional speech grouped by emotion valence. The final part of our research focused on varying speaker model structure by determining the number of components in a Gaussian mixture based on subtractive clustering. The number of components was automatically determined from the training utterances for each speaker using subtractive clustering. We induced theoretical dependencies of subtractive clustering parameters and the number of feature vectors in the training sample, feature vector dimensionality, and their distribution. Our results showed that speakers can be successfully modeled with fewer components in a Gaussian mixture, paving the way for further research into the application of subtractive clustering for speaker model training.