The successful incorporation of emotional intelligence into advanced artificial intelligence systems is anchored in the accurate identification of emotional states, with the paralinguistic elements of speech emerging as a particularly critical source of information about the speaker's emotional condition. This paper conducts a comparative review of speech signal characteristics and classification techniques commonly employed to tackle the challenge of automatically recognizing speakers' emotional states, followed by an exploration of ways to enhance the performance of systems for automatic speech emotion recognition. Discrete hidden Markov models were enhanced using the QQ plot to identify the codevectors for vector quantization, and additional model enhancements were also contemplated. The potential for a more accurate depiction of the speech signal was investigated, with the analysis encompassing a plethora of features from various categories. The creation of large feature sets necessitates dimensionality reduction, where an alternative method based on the Fibonacci number sequence was scrutinized, in addition to established methods. Ultimately, the potential for merging the benefits of diverse approaches into a unified system for automatic speech emotion recognition is considered, proposing a parallel multiclassifier structure with a combinatorial rule that utilizes information about classifiers' characteristics, in addition to the classification results of individual ensemble classifiers. A suggestion is also put forth for the automatic creation of a classifier ensemble of any size using dimensionality reduction based on the Fibonacci number sequence.