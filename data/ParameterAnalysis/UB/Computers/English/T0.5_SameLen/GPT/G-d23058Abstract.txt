Emotional speech modeling is not often incorporated in contemporary speaker recognition research. The primary obstacle for a speaker recognition system is the variation in a speaker's emotional state during system training and usage. In our study, we analyzed various speaker modeling and classification algorithms. We evaluated Gaussian mixture models, a basic technique in speaker recognition, and i-vectors, a cutting-edge technique for commercial applications, in our experiments. Our findings indicated that alternating the emotional content of speech and the quantity of sentences for speaker model training enhances system resilience in recognizing speakers from both neutral and emotionally affected speech. Additionally, we explored model configuration variation by developing three models for each speaker. Each model was trained with suitable emotional speech categorized by emotion valence. The final segment of our research focused on modifying speaker model structure by determining the number of components in a Gaussian mixture using subtractive clustering. The component number was automatically derived from the training utterances for each speaker through subtractive clustering. We established theoretical correlations between subtractive clustering parameters, the quantity of feature vectors in the training sample, feature vector dimensionality, and their distribution. Our results demonstrated that speakers can be successfully modeled with fewer Gaussian mixture components, paving the way for additional research on the application of subtractive clustering for speaker model training.