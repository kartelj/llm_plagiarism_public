Emotional speech, a scarcely explored phenomenon, is contemporarily dismissed in speaker recognition research. The disparity within a speaker's emotional state during training and utilization phases of recognition systems presents a primary challenge. Our study assessed various speaker modeling and classification techniques. Gaussian mixture models, a cornerstone technique in speaker recognition, and I-vectors, a modern method for commercial use, were evaluated through our experiments. We demonstrated that system robustness in recognizing speakers from both neutral and emotionally charged speech can be enhanced by altering emotional content and sentence quantity during speaker model training. We further explored the effect of altering model configuration by creating three separate models for each speaker. Each of these models was trained using carefully sorted emotional speech based on emotion valence. The concluding research segment was dedicated to modulating speaker model structure by determining Gaussian mixture components via subtractive clustering. The components number was automatically determined from each speaker's training statements using subtractive clustering.  Theoretical connections between subtractive clustering parameters, training sample feature vector numbers, feature vector dimension, and distribution were drawn up. The results indicated that fewer Gaussian mixture components can successfully model a speaker, leaving room for continued subtractive clustering application research in speaker model training.