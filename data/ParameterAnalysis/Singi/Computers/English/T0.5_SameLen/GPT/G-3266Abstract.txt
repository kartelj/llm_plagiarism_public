Recent studies in automatic speaker recognition have revealed that deep learning neural networks-based methods outperform other statistical classifiers. However, these methods often necessitate the tuning of a substantial number of parameters. This thesis aims to demonstrate that the correct selection of parameter values can significantly enhance the speaker recognition performance of deep learning neural networks-based methods. The study presented introduces a strategy for automatic speaker recognition that utilises deep neural networks and the stochastic gradient descent algorithm, focusing specifically on three of the algorithm's parameters: the learning rate, and the dropout rates of the hidden and input layers. Special consideration was given to the issue of speaker recognition in noisy environments. Consequently, two experiments were carried out as part of this thesis. The first experiment aimed to prove that optimizing the observed parameters of the stochastic gradient descent algorithm can enhance speaker recognition performance in the absence of noise. This experiment was conducted in two stages. In the first stage, the recognition rate was monitored while varying the hidden layer dropout rate and the learning rate, with the input layer dropout rate remaining constant. In the second stage, the recognition rate was monitored while varying the input layer dropout rate and the learning rate, with the hidden layer dropout rate remaining constant. The second experiment aimed to demonstrate that optimizing the observed parameters of the stochastic gradient descent algorithm can enhance speaker recognition performance even in noisy environments. As such, various noise levels were artificially added to the original speech signal. The results obtained indicate that dropout optimization can significantly improve the performance of the stochastic gradient descent method in automatic speaker recognition, even in noisy environments. It was also found that choosing the correct learning rate value is crucial, as some values can negatively impact the method's performance.