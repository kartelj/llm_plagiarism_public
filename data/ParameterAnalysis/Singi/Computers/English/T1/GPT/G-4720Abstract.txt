The perpetuation of culture shapes society, and in turn, society is formed through the evolution and advancement of cultural and intellectual knowledge. Education plays a crucial role in this developmental process. A fundamental component of education involves assessing both theoretical understanding and practical proficiencies. Out of all the disciplines in natural and technical sciences, computer science frequently institutes assessments to gauge students' practical abilities. A range of assessment techniques are employed in education, and there are ongoing efforts to refine these methods to ensure an accurate and unbiased evaluation. There are various methods mentioned in literature, some obsolete and others still in use, with closed question type tests and practical work reviews and evaluations being the more common ones. While tests effectively measure knowledge, the evaluation of practical work helps ascertain the level of practical skills. Both methods can be executed manually or automatically, with automated methods being set up to function without the involvement of examiners. Given professional experience, manual student assessment, regardless of the method used, poses challenges in maintaining impartiality and precision, making a case for automated evaluations. However, applying these methods to assess both theoretical knowledge and practical skills is problematic due to their incompatibility. This paper scrutinizes the most common issues in literature reviews, aiming to devise a method for accurately and objectively evaluating students' practical skills in programming-related subjects. During the four-year research period, these proprietary solutions were used to evaluate two cohorts of students: one using an online multiple-choice test for knowledge testing and the other studying the accuracy of students' practical assignments. This research paper answers several questions through the analysis of data gathered from these two assessment methods. Both qualitative and quantitative research techniques were employed to analyze the data. Furthermore, this paper includes case studies of the assessment methods and tools used for evaluations during the study period. The feasability of developing an automated tool to supplant teachers as examiners by automating the review and grading of practical assignments is explored, as is the comparison of assessment results from both methods. Implications and conclusions drawn from the analyzed data and research are discussed. Ultimately, the study proposes an approach to objectively and accurately assess practical skills in programming-based computer science subjects at the undergraduate level. It also presents potential solutions to achieve this objective. This research holds both academic and practical value.