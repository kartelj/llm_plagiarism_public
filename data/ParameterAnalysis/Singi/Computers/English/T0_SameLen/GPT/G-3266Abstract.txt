Recent studies in automatic speaker recognition have demonstrated that deep learning neural network-based methods outperform other statistical classifiers. However, these methods often necessitate the tuning of a substantial number of parameters. This thesis aims to demonstrate that choosing the correct parameter values can greatly enhance the speaker recognition performance of deep learning neural network-based methods. The study presented introduces a method for automatic speaker recognition that utilizes deep neural networks and the stochastic gradient descent algorithm, with a specific focus on three parameters: the learning rate, and the dropout rates of the hidden and input layers. Special consideration was given to the issue of speaker recognition in noisy environments. As such, two experiments were carried out as part of this thesis. The first experiment aimed to show that optimizing the parameters of the stochastic gradient descent algorithm can enhance speaker recognition performance in the absence of noise. This experiment was conducted in two stages. In the first stage, the recognition rate was observed while varying the hidden layer dropout rate and the learning rate, with the input layer dropout rate remaining constant. In the second stage, the recognition rate was observed while varying the input layer dropout rate and the learning rate, with the hidden layer dropout rate remaining constant. The second experiment aimed to demonstrate that optimizing the parameters of the stochastic gradient descent algorithm can enhance speaker recognition performance even in noisy environments. Therefore, various levels of noise were artificially introduced to the original speech signal. The results obtained indicate that dropout optimization can significantly improve the performance of the stochastic gradient descent method in automatic speaker recognition, even in noisy environments. It was also found that choosing the correct learning rate is crucial, as certain values of this parameter can negatively impact the method's performance.