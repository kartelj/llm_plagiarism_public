This thesis examines techniques for more precise identification of video content altered with the Deepfake method. Earlier models designed for detecting video alterations via the Deepfake method, along with their models and parameters, were scrutinised. These models include XceptionNet, EfficientNetB, and EfficientNetV. During the retraining phase, changes were made to the SingleDLCNN network, the fold number, and the Hold-out technique parameters. A DataSet comprising over 6000 files was utilised, with the bulk used for neural network training and the remainder for testing and validation. Cross-Validation was employed to obtain the best outcomes, and accuracy was enhanced through weight averaging, or weight optimisation. The outcomes for all trained models are displayed. The most successful result was obtained using the EfficientNetbB4, with 96.8% (FAR = 5.97%). We are confident that the result achieved validates the quality of this training model. In future endeavours, we aim to refine the model and ultimately bring it to market.