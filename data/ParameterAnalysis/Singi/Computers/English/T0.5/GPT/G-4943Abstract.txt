This thesis scrutinises the techniques for enhanced precision in detecting video content altered with the Deepfake technology. It examines prior models designed for identifying video alterations via the Deepfake method, along with their individual models and parameters. These models include XceptionNet, EfficientNetB, and EfficientNetV. During the retraining process, modifications were made to the SingleDLCNN network, the fold number, and the parameters for the Hold-out technique. A DataSet comprising over 6000 files was utilised, with the bulk of them being used for training the neural network and the remainder for testing and validation. The best results were derived using Cross-Validation, and accuracy was boosted through weight averaging, or weight optimisation. The outcomes for all trained models are outlined. The most successful result was obtained using the EfficientNetbB4, with an accuracy of 96.8% (FAR = 5.97%). We are confident that the result achieved validates the efficacy of this training model. In future, we aim to refine the model and eventually bring it to market.