Recent studies in automatic speaker recognition have demonstrated that deep learning neural networks outperform other statistical classifiers. However, these methods often necessitate the adjustment of many parameters. This thesis aims to illustrate how the careful selection of parameter values can significantly enhance the speaker recognition performance of deep learning neural network-based methods. The study presented here introduces a deep neural network-based automatic speaker recognition approach, which utilizes the stochastic gradient descent algorithm. The focus is on three parameters of this algorithm: the learning rate, and the dropout rates of the hidden and input layers. The study also explores speaker recognition in noisy environments. To this end, two experiments were carried out. The first sought to show that optimizing the parameters of the stochastic gradient descent algorithm can improve speaker recognition performance in noise-free environments. This experiment was divided into two phases, each observing the recognition rate under different parameter variations. The second experiment aimed to demonstrate that these optimizations can also improve speaker recognition in noisy environments, with various noise levels artificially applied to the original speech signal. The results indicate that dropout optimization can significantly improve the performance of the stochastic gradient descent method in automatic speaker recognition, even in noisy environments. The results also highlight the importance of selecting an appropriate learning rate, as certain values can negatively impact performance.