Recent studies in the area of automated voice recognition have demonstrated that techniques utilizing deep learning neural networks outperform other statistical classifiers. However, these techniques frequently necessitate the tuning of a considerable number of parameters. This dissertation aims to illustrate how selecting the correct parameter values can drastically enhance voice recognition performance when using deep learning neural networks. The study discussed espouses an approach to automated voice recognition using deep neural networks and the stochastic gradient descent algorithm, with a particular emphasis on its three parameters: learning rate and both the hidden and input layer dropout rates. Additional research focused on the problem of voice recognition in noisy environments. As such, this dissertation encompassed two experiments. The aim of the first experiment was to show that tuning the parameters of the stochastic gradient descent algorithm improves voice recognition performance in the absence of noise. This experiment spanned two phases; the initial phase observed the recognition rate with the hidden layer dropout and learning rates altered, and the input layer dropout rate held constant, while the subsequent phase observed the recognition rate with changes to the input layer dropout and learning rates, while the hidden layer dropout remained steady. The second experiment was tasked with showing similar improvements in voice recognition in noisy environments and involved varying levels of artificial noises applied to the initial speech signal. The acquired results prove that optimizing dropout can drastically augment the efficiency of the stochastic gradient descent technique in automated voice recognition, even amid noise. It was also proven that accurately choosing the learning rate is crucial as incorrect adjustments can detrimentally impact the techniqueâ€™s performance.