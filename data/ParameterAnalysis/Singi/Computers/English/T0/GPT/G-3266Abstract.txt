Recent studies in automatic speaker recognition have demonstrated that deep learning neural networks outperform other statistical classifiers. However, these methods often necessitate the adjustment of a large number of parameters. This thesis aims to demonstrate that the careful selection of parameter values can significantly enhance the performance of speaker recognition methods based on deep learning neural networks. The study presented introduces a method for automatic speaker recognition that utilizes deep neural networks and the stochastic gradient descent algorithm, with a particular focus on three parameters: the learning rate, and the dropout rates of the hidden and input layers. The study also investigates speaker recognition in noisy environments. Two experiments were conducted for this thesis. The first aimed to show that optimizing the parameters of the stochastic gradient descent algorithm can improve speaker recognition performance in the absence of noise. This experiment was conducted in two stages, with the first observing the recognition rate as the hidden layer dropout rate and the learning rate were varied, while the input layer dropout rate remained constant. The second stage observed the recognition rate as the input layer dropout rate and learning rate were varied, with the hidden layer dropout rate remaining constant. The second experiment aimed to demonstrate that parameter optimization can improve speaker recognition performance even in noisy environments, with different levels of noise artificially applied to the original speech signal. The results indicate that dropout optimization can significantly improve the performance of the stochastic gradient descent method in automatic speaker recognition, even in noisy conditions. The study also found that choosing the correct learning rate is crucial, as certain values can negatively impact the method's performance.